{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Model for Book Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.chat_models import init_chat_model\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "model = init_chat_model(\"google_genai:gemini-2.5-flash-lite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Thank you for the test message! I've received it.\\n\\nAs Gemini 2.5, I'm ready to process your requests and engage in conversations. Please feel free to ask me anything, give me tasks, or provide me with information. I'm here to help!\" additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash-lite', 'safety_ratings': [], 'model_provider': 'google_genai'} id='lc_run--565b915e-1ecc-4ecf-8f86-a75ae621eaa2-0' usage_metadata={'input_tokens': 15, 'output_tokens': 59, 'total_tokens': 74, 'input_token_details': {'cache_read': 0}}\n"
     ]
    }
   ],
   "source": [
    "#send a request to the model\n",
    "response = model.invoke(\"This is a test message to the Gemini 2.5 model.\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"I am a large language model, trained by Google. I don't have access to past conversations. Therefore, I cannot tell you what you asked me before.\" additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash-lite', 'safety_ratings': [], 'model_provider': 'google_genai'} id='lc_run--50bb29b5-3ae5-4ff0-9931-c3df5ad71414-0' usage_metadata={'input_tokens': 8, 'output_tokens': 33, 'total_tokens': 41, 'input_token_details': {'cache_read': 0}}\n"
     ]
    }
   ],
   "source": [
    "#Check whether these invokations have a state\n",
    "response = model.invoke(\"What did I ask you before?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding model: models/embedding-001\n",
      "Embedding model: models/text-embedding-004\n",
      "Embedding model: models/gemini-embedding-exp-03-07\n",
      "Embedding model: models/gemini-embedding-exp\n",
      "Embedding model: models/gemini-embedding-001\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "url = f\"https://generativelanguage.googleapis.com/v1beta/models?key={os.environ['GOOGLE_API_KEY']}\"\n",
    "\n",
    "response = requests.get(url)\n",
    "models = response.json()\n",
    "\n",
    "for model_i in models.get('models', []):\n",
    "    if 'embedContent' in model_i.get('supportedGenerationMethods', []):\n",
    "        print(f\"Embedding model: {model_i['name']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "#embeddings = GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-001\")\n",
    "\n",
    "#Free tier embedding model\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"text-embedding-004\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Similarity Comparison: English vs Spanish =====\n",
      "\n",
      "--- Similar action (sleeping), different animal ---\n",
      "EN (The cat is sleeping.) vs (The dog is sleeping.): 0.8296\n",
      "ES (El gato está durmiendo.) vs (El perro está durmiendo.): 0.8640\n",
      "Difference (EN - ES): -0.0344\n",
      "\n",
      "--- Totally different topics ---\n",
      "EN (The rocket is launching into space.) vs (I am cooking pasta in the kitchen.): 0.3719\n",
      "ES (El cohete está despegando hacia el espacio.) vs (Estoy cocinando pasta en la cocina.): 0.5021\n",
      "Difference (EN - ES): -0.1303\n",
      "\n",
      "--- Similar topic (economy/stock market) ---\n",
      "EN (The economy is experiencing significant growth this quarter.) vs (The stock market reached an all-time high today.): 0.4961\n",
      "ES (La economía está experimentando un crecimiento significativo este trimestre.) vs (El mercado de valores alcanzó un máximo histórico hoy.): 0.6235\n",
      "Difference (EN - ES): -0.1274\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "# ---- Dataset ----\n",
    "eng = [\n",
    "    \"The cat is sleeping.\",\n",
    "    \"The dog is sleeping.\",\n",
    "    \"The rocket is launching into space.\",\n",
    "    \"I am cooking pasta in the kitchen.\",\n",
    "    \"The economy is experiencing significant growth this quarter.\",\n",
    "    \"The stock market reached an all-time high today.\",\n",
    "]\n",
    "\n",
    "spa = [\n",
    "    \"El gato está durmiendo.\",\n",
    "    \"El perro está durmiendo.\",\n",
    "    \"El cohete está despegando hacia el espacio.\",\n",
    "    \"Estoy cocinando pasta en la cocina.\",\n",
    "    \"La economía está experimentando un crecimiento significativo este trimestre.\",\n",
    "    \"El mercado de valores alcanzó un máximo histórico hoy.\",\n",
    "]\n",
    "\n",
    "# ---- Embed ----\n",
    "emb_eng = [embeddings.embed_query(s) for s in eng]\n",
    "emb_spa = [embeddings.embed_query(s) for s in spa]\n",
    "\n",
    "# ---- Similarity matrices ----\n",
    "sim_eng = cosine_similarity(emb_eng)\n",
    "sim_spa = cosine_similarity(emb_spa)\n",
    "\n",
    "# ---- Compare the key pairs ----\n",
    "pairs = [\n",
    "    (0, 1, \"Similar action (sleeping), different animal\"),\n",
    "    (2, 3, \"Totally different topics\"),\n",
    "    (4, 5, \"Similar topic (economy/stock market)\"),\n",
    "]\n",
    "\n",
    "print(\"\\n===== Similarity Comparison: English vs Spanish =====\\n\")\n",
    "\n",
    "for i, j, label in pairs:\n",
    "    print(f\"--- {label} ---\")\n",
    "    print(f\"EN ({eng[i]}) vs ({eng[j]}): {sim_eng[i][j]:.4f}\")\n",
    "    print(f\"ES ({spa[i]}) vs ({spa[j]}): {sim_spa[i][j]:.4f}\")\n",
    "    diff = sim_eng[i][j] - sim_spa[i][j]\n",
    "    print(f\"Difference (EN - ES): {diff:.4f}\\n\")\n",
    "\n",
    "#Looks like the model captures similarities somewhat consistently across both languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"example_collection\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"../vectors/chroma_langchain_db\",  # Where to save data locally\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters: 3153561\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "# Only keep post title, headers, and content from the full HTML.\n",
    "loader = TextLoader(\n",
    "    \"../data/Guerra_y_paz_cleaned.txt\",\n",
    "    encoding=\"utf-8\",\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "assert len(docs) == 1\n",
    "print(f\"Total characters: {len(docs[0].page_content)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "—Eh bien, mon prince, Génova y Lucca ya no son más que posesiones de la familia Bonaparte. No, le prevengo que si usted no me dice que estamos en plena guerra, si vuelve a permitirse paliar todas las infamias, todas las atrocidades de ese Anticristo (le doy mi palabra de que así lo considero), a usted ya no lo conozco, no es usted mi amigo, no es mi devoto esclavo, como dice. Ea, bienvenido, bienvenido. Veo que lo he asustado. Siéntese y charlemos.\n",
      "Con tales palabras, Anna Pávlovna Scherer, dama\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.documents.base.Document"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split blog post into 4140 sub-documents.\n",
      "Split blog post into 4387 sub-documents.\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter_0 = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,  # chunk size (characters)\n",
    "    chunk_overlap=0,  # chunk overlap (characters)\n",
    "    add_start_index=True,  # track index in original document\n",
    ")\n",
    "\n",
    "text_splitter_200 = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,  # chunk size (characters)\n",
    "    chunk_overlap=200,  # chunk overlap (characters)\n",
    "    add_start_index=True,  # track index in original document\n",
    ")\n",
    "all_splits_0 = text_splitter_0.split_documents(docs)\n",
    "all_splits_200 = text_splitter_200.split_documents(docs)\n",
    "print(f\"Split blog post into {len(all_splits_0)} sub-documents.\")\n",
    "print(f\"Split blog post into {len(all_splits_200)} sub-documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_0_200 = all_splits_200[0].page_content\n",
    "split_1_200 = all_splits_200[1].page_content\n",
    "split_2_200 = all_splits_200[2].page_content\n",
    "split_3_200 = all_splits_200[3].page_content\n",
    "split_4_200 = all_splits_200[4].page_content\n",
    "\n",
    "split_0_0 = all_splits_0[0].page_content\n",
    "split_1_0 = all_splits_0[1].page_content\n",
    "split_2_0 = all_splits_0[2].page_content\n",
    "split_3_0 = all_splits_0[3].page_content\n",
    "split_4_0 = all_splits_0[4].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m split_1_0 == split_1_200\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m split_2_0 == split_2_200\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m split_3_0 == split_3_200\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m split_4_0 == split_4_200\n",
      "\u001b[31mAssertionError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "assert split_0_0 == split_0_200\n",
    "assert split_1_0 == split_1_200\n",
    "assert split_2_0 == split_2_200\n",
    "assert split_3_0 == split_3_200\n",
    "assert split_4_0 == split_4_200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of split 0: 200 overlap 974\n",
      "Len of split 0: 000 overlap 974\n",
      "Len of split 1: 200 overlap 874\n",
      "Len of split 1: 000 overlap 874\n",
      "Len of split 2: 200 overlap 944\n",
      "Len of split 2: 000 overlap 944\n",
      "Len of split 3: 200 overlap 532\n",
      "Len of split 3: 000 overlap 967\n",
      "Len of split 4: 200 overlap 541\n",
      "Len of split 4: 000 overlap 999\n"
     ]
    }
   ],
   "source": [
    "print(\"Len of split 0: 200 overlap\", len(split_0_200))\n",
    "print(\"Len of split 0: 000 overlap\", len(split_0_0))\n",
    "\n",
    "print(\"Len of split 1: 200 overlap\", len(split_1_200))\n",
    "print(\"Len of split 1: 000 overlap\", len(split_1_0))\n",
    "\n",
    "print(\"Len of split 2: 200 overlap\", len(split_2_200))\n",
    "print(\"Len of split 2: 000 overlap\", len(split_2_0))\n",
    "\n",
    "print(\"Len of split 3: 200 overlap\", len(split_3_200))\n",
    "print(\"Len of split 3: 000 overlap\", len(split_3_0))\n",
    "\n",
    "print(\"Len of split 4: 200 overlap\", len(split_4_200))\n",
    "print(\"Len of split 4: 000 overlap\", len(split_4_0))\n",
    "\n",
    "# There seems to be a faulty behavior with the implementation of this splitter.\n",
    "# The overlaps seem to only be launched when the original split is smaller than chunk_size - chunk_overlap or something.\n",
    "# Need to investigate further. https://stackoverflow.com/questions/76681318/why-is-recursivecharactertextsplitter-not-giving-any-chunk-overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "GoogleGenerativeAIError",
     "evalue": "Error embedding content: 404 models/text-multilingual-embedding-002 is not found for API version v1beta, or is not supported for embedContent. Call ListModels to see the list of available models and their supported methods.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotFound\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\josep\\anaconda3\\envs\\py311\\Lib\\site-packages\\langchain_google_genai\\embeddings.py:306\u001b[39m, in \u001b[36mGoogleGenerativeAIEmbeddings.embed_documents\u001b[39m\u001b[34m(self, texts, batch_size, task_type, titles, output_dimensionality)\u001b[39m\n\u001b[32m    305\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m306\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbatch_embed_contents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    307\u001b[39m \u001b[43m        \u001b[49m\u001b[43mBatchEmbedContentsRequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequests\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequests\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    308\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    309\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\josep\\anaconda3\\envs\\py311\\Lib\\site-packages\\google\\ai\\generativelanguage_v1beta\\services\\generative_service\\client.py:1438\u001b[39m, in \u001b[36mGenerativeServiceClient.batch_embed_contents\u001b[39m\u001b[34m(self, request, model, requests, retry, timeout, metadata)\u001b[39m\n\u001b[32m   1437\u001b[39m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1438\u001b[39m response = \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1439\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1440\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1441\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1442\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1443\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1445\u001b[39m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\josep\\anaconda3\\envs\\py311\\Lib\\site-packages\\google\\api_core\\gapic_v1\\method.py:131\u001b[39m, in \u001b[36m_GapicCallable.__call__\u001b[39m\u001b[34m(self, timeout, retry, compression, *args, **kwargs)\u001b[39m\n\u001b[32m    129\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mcompression\u001b[39m\u001b[33m\"\u001b[39m] = compression\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\josep\\anaconda3\\envs\\py311\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:294\u001b[39m, in \u001b[36mRetry.__call__.<locals>.retry_wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    291\u001b[39m sleep_generator = exponential_sleep_generator(\n\u001b[32m    292\u001b[39m     \u001b[38;5;28mself\u001b[39m._initial, \u001b[38;5;28mself\u001b[39m._maximum, multiplier=\u001b[38;5;28mself\u001b[39m._multiplier\n\u001b[32m    293\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m294\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m=\u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\josep\\anaconda3\\envs\\py311\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:156\u001b[39m, in \u001b[36mretry_target\u001b[39m\u001b[34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[39m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    155\u001b[39m     \u001b[38;5;66;03m# defer to shared logic for handling errors\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m     next_sleep = \u001b[43m_retry_error_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    158\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdeadline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[43m        \u001b[49m\u001b[43msleep_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[43m        \u001b[49m\u001b[43merror_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpredicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m        \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexception_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# if exception not raised, sleep before next attempt\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\josep\\anaconda3\\envs\\py311\\Lib\\site-packages\\google\\api_core\\retry\\retry_base.py:214\u001b[39m, in \u001b[36m_retry_error_helper\u001b[39m\u001b[34m(exc, deadline, sleep_iterator, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[39m\n\u001b[32m    209\u001b[39m     final_exc, source_exc = exc_factory_fn(\n\u001b[32m    210\u001b[39m         error_list,\n\u001b[32m    211\u001b[39m         RetryFailureReason.NON_RETRYABLE_ERROR,\n\u001b[32m    212\u001b[39m         original_timeout,\n\u001b[32m    213\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m214\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m final_exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msource_exc\u001b[39;00m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m on_error_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\josep\\anaconda3\\envs\\py311\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:147\u001b[39m, in \u001b[36mretry_target\u001b[39m\u001b[34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[39m\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m     result = \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    148\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m inspect.isawaitable(result):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\josep\\anaconda3\\envs\\py311\\Lib\\site-packages\\google\\api_core\\timeout.py:130\u001b[39m, in \u001b[36mTimeToDeadlineTimeout.__call__.<locals>.func_with_timeout\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    128\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m] = remaining_timeout\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\josep\\anaconda3\\envs\\py311\\Lib\\site-packages\\google\\api_core\\grpc_helpers.py:77\u001b[39m, in \u001b[36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m grpc.RpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions.from_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n",
      "\u001b[31mNotFound\u001b[39m: 404 models/text-multilingual-embedding-002 is not found for API version v1beta, or is not supported for embedContent. Call ListModels to see the list of available models and their supported methods.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mGoogleGenerativeAIError\u001b[39m                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m document_ids = \u001b[43mvector_store\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m=\u001b[49m\u001b[43mall_splits_200\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(document_ids[:\u001b[32m3\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\josep\\anaconda3\\envs\\py311\\Lib\\site-packages\\langchain_core\\vectorstores\\base.py:257\u001b[39m, in \u001b[36mVectorStore.add_documents\u001b[39m\u001b[34m(self, documents, **kwargs)\u001b[39m\n\u001b[32m    255\u001b[39m     texts = [doc.page_content \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[32m    256\u001b[39m     metadatas = [doc.metadata \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[32m--> \u001b[39m\u001b[32m257\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madd_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    258\u001b[39m msg = (\n\u001b[32m    259\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m`add_documents` and `add_texts` has not been implemented \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    260\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mfor \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    261\u001b[39m )\n\u001b[32m    262\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(msg)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\josep\\anaconda3\\envs\\py311\\Lib\\site-packages\\langchain_chroma\\vectorstores.py:620\u001b[39m, in \u001b[36mChroma.add_texts\u001b[39m\u001b[34m(self, texts, metadatas, ids, **kwargs)\u001b[39m\n\u001b[32m    618\u001b[39m texts = \u001b[38;5;28mlist\u001b[39m(texts)\n\u001b[32m    619\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._embedding_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m     embeddings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_embedding_function\u001b[49m\u001b[43m.\u001b[49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    621\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m metadatas:\n\u001b[32m    622\u001b[39m     \u001b[38;5;66;03m# fill metadatas with empty dicts if somebody\u001b[39;00m\n\u001b[32m    623\u001b[39m     \u001b[38;5;66;03m# did not specify metadata for all texts\u001b[39;00m\n\u001b[32m    624\u001b[39m     length_diff = \u001b[38;5;28mlen\u001b[39m(texts) - \u001b[38;5;28mlen\u001b[39m(metadatas)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\josep\\anaconda3\\envs\\py311\\Lib\\site-packages\\langchain_google_genai\\embeddings.py:311\u001b[39m, in \u001b[36mGoogleGenerativeAIEmbeddings.embed_documents\u001b[39m\u001b[34m(self, texts, batch_size, task_type, titles, output_dimensionality)\u001b[39m\n\u001b[32m    309\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    310\u001b[39m         msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError embedding content: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m311\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m GoogleGenerativeAIError(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    312\u001b[39m     embeddings.extend([\u001b[38;5;28mlist\u001b[39m(e.values) \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m result.embeddings])\n\u001b[32m    313\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m embeddings\n",
      "\u001b[31mGoogleGenerativeAIError\u001b[39m: Error embedding content: 404 models/text-multilingual-embedding-002 is not found for API version v1beta, or is not supported for embedContent. Call ListModels to see the list of available models and their supported methods."
     ]
    }
   ],
   "source": [
    "document_ids = vector_store.add_documents(documents=all_splits_200)\n",
    "\n",
    "print(document_ids[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now you can query it directly\n",
    "results = vector_store.similarity_search(\"¿Quien es Napoleón?\", k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='0aebe282-f92e-4235-a907-bc77797fdf74', metadata={'source': '../data/Guerra_y_paz_cleaned.txt', 'start_index': 2931087}, page_content='Y a medida que esas fuerzas se multiplican aumenta más el prestigio del hombre que lo dirige y más justificadas se consideran sus acciones. Durante el período preparatorio de diez años que precede al gran movimiento, ese hombre se emparenta con todas las cabezas coronadas de Europa. Los desbancados dueños del mundo no pueden oponer ideal alguno razonable al insensato ideal de gloria y grandeza de Napoleón. Uno tras otro, se apresuran a demostrarle lo poco que valen. El rey de Prusia envía a su esposa para lograr el favor del gran hombre. El emperador de Austria considera un honor que ese hombre acepte en su lecho a la hija de los Césares. El Papa, custodio de los santuarios de los pueblos, pone en juego la religión para exaltarlo. No es Napoleón solo quien se prepara para asumir su papel; los que lo rodean, hasta más que él, lo preparan para que acepte la responsabilidad de todo cuanto se hace y ha de hacerse. Los hechos de ese hombre, sus crímenes, hasta sus más pequeños engaños, se'),\n",
       " Document(id='60d35ecb-3575-4ec2-a06e-b1d2adaf835e', metadata={'source': '../data/Guerra_y_paz_cleaned.txt', 'start_index': 2038050}, page_content='Se interesaba por bagatelas; bromeó acerca de la afición a los viajes de De Beausset y charló negligente como hace un célebre cirujano seguro de sí mismo mientras se arremanga y pone la bata y atan al enfermo en la mesa de operaciones. “Todo está en mis manos y lo tengo claro y definido en mi cabeza. Cuando llegue el momento de actuar, lo haré como ningún otro; ahora puedo bromear, y cuanto más bromee y más tranquilo esté, más seguros, tranquilos y admirados de mi genio debéis estar vosotros.\\nCuando terminó su segundo vaso de ponche, Napoleón se retiró a descansar, a la espera del grave asunto que, según le parecía, lo esperaba al día siguiente.'),\n",
       " Document(id='bec0fb4b-cf9b-40e0-9c7b-3b8eea48ee48', metadata={'source': '../data/Guerra_y_paz_cleaned.txt', 'start_index': 40415}, page_content='Pierre miraba triunfalmente a los oyentes por encima de sus anteojos.\\n—Digo eso— prosiguió con desesperada decisión —porque los Borbones han huido de la revolución dejando al pueblo entregado a la anarquía; sólo Napoleón supo comprender la revolución y vencerla. Por eso, y por el bien común, no podía detenerse ante la vida de un solo hombre.\\n—¿No quiere pasar a esa otra mesa?— dijo Anna Pávlovna.\\nPero, sin contestar, Pierre continuó su discurso, cada vez más animado.\\n—Sí, Napoleón es grande porque supo ponerse por encima de la revolución, reprimiendo sus abusos y tomando cuanto tenía de bueno: la igualdad de los ciudadanos, la libertad de palabra y de prensa, y tan sólo por eso conquistó el poder.\\n—Así sería si al tomar el poder, sin valerse del asesinato, lo hubiera devuelto al rey legítimo— dijo el vizconde; —entonces yo lo llamaría gran hombre.'),\n",
       " Document(id='455f9445-25bf-4d30-9b56-d3685f0a2329', metadata={'start_index': 3053526, 'source': '../data/Guerra_y_paz_cleaned.txt'}, page_content='Para encontrar respuesta a tales preguntas nos dirigimos a la historia, la ciencia cuyo objeto es el estudio de las naciones y la humanidad.\\nSi la historia mantuviera las viejas concepciones diría: la divinidad, para recompensar o castigar a su pueblo, dio a Napoleón el poder y guió su voluntad hasta la consecución de sus divinos fines. Esa respuesta sería completa y clara. Puede creerse o no que Napoleón tuviera una misión sagrada; para quien lo cree, todo resulta comprensible en la historia de ese período y no halla contradicción alguna.\\nPero la nueva ciencia histórica no puede contestar así. La ciencia no admite las concepciones antiguas sobre la directa participación de la divinidad en las acciones humanas; por eso, debe proporcionarnos otras respuestas.\\n¿Queréis saber qué significa ese movimiento, de dónde procede y qué fuerza lo engendró? La nueva ciencia histórica responde así:'),\n",
       " Document(id='85362cf5-cee0-4f09-8deb-99cbab46d8e6', metadata={'start_index': 2261557, 'source': '../data/Guerra_y_paz_cleaned.txt'}, page_content='A Napoleón le parecía que el sentido principal de cuanto ocurría se debía a su lucha personal con Alejandro.\\n“Desde las alturas del Kremlin, si aquello es el Kremlin, les daré leyes justas. Les mostraré la grandeza de la verdadera civilización; obligaré a generaciones enteras de boyardos a recordar con cariño el nombre de su conquistador. Diré a su delegación que nunca he querido ni quiero la guerra, que sólo he combatido la política engañosa de su Corte, que amo y respeto a Alejandro y aceptaré en Moscú condiciones de paz dignas de mí y de mis pueblos. No quiero aprovecharme del éxito de la guerra para humillar a un Emperador al que estimo. Boyardos —les diré—, yo no quiero la guerra, deseo la paz y la felicidad de todos mis súbditos. Sé, además, que la presencia de esos hombres me inspirará, y les hablaré como lo hago siempre: con precisión, solemnidad y grandeza… Pero ¿será verdad que estoy en Moscú? ¡Sí, ahí está!”\\n—Qu’on m’amène les boyards— dijo a su escolta.')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "\n",
    "@tool(response_format=\"content_and_artifact\")\n",
    "def get_similar_documents(query: str):\n",
    "    \"\"\"Retrieve passages from the book to help answer a question.\"\"\"\n",
    "    retrieved_docs = vector_store.similarity_search(query, k=3)\n",
    "    serialized = \"\\n\\n\".join(\n",
    "        (f\"Source: {doc.metadata}\\nContent: {doc.page_content}\")\n",
    "        for doc in retrieved_docs\n",
    "    )\n",
    "    return serialized, retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "\n",
    "\n",
    "tools = [get_similar_documents]\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are an AI assistant for answering questions about a book or document. \"\n",
    "    \"You have access to a tool that retrieves context from a database of books you have available to read.\"\n",
    "    \"Use the tool to help answer user queries.\"\n",
    "    \"If you use the tool, be sure to cite the sources of the information you provide.\"\n",
    "    \"At the end of the explanation make a little summarty starting 'In summary,...'\"\n",
    ")\n",
    "agent = create_agent(model=model, tools=tools, system_prompt=system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_google_genai.chat_models.ChatGoogleGenerativeAIInput"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.input_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "¿Quién es Napoleón en el libro Guerra y Paz?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  get_similar_documents (b433bbaf-8cdd-4070-94a1-604e128206d4)\n",
      " Call ID: b433bbaf-8cdd-4070-94a1-604e128206d4\n",
      "  Args:\n",
      "    query: Napoleón en Guerra y Paz\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: get_similar_documents\n",
      "\n",
      "Source: {'source': '../data/Guerra_y_paz_cleaned.txt', 'start_index': 694379}\n",
      "Content: Aquél era para Napoleón un día solemne: el aniversario de su coronación. Antes del alba había dormido unas horas, y ahora, tranquilo, jovial, descansado, en esa feliz disposición de ánimo en la que todo parece posible y todo se consigue, había montado a caballo para dirigirse al campo de batalla. Permanecía inmóvil, mirando hacia las colinas que se iban liberando de la niebla; su rostro frío reflejaba aquel matiz peculiar de seguridad en sí mismo, la seguridad de merecer la felicidad que sólo se encuentra en la sonrisa del muchacho enamorado y feliz. Los mariscales permanecían detrás de él sin atreverse a distraer su atención. El Emperador contemplaba, ya los altos de Pratzen, ya el sol que emergía de entre la niebla.\n",
      "\n",
      "Source: {'source': '../data/Guerra_y_paz_cleaned.txt', 'start_index': 683018}\n",
      "Content: ¡Que no se rompan las filas con el pretexto de retirar a los heridos! Cada uno debe compenetrarse bien con la idea de que es necesario vencer a esos mercenarios de Inglaterra, animados de tanto odio hacia nuestra nación. Esta victoria pondrá fin a la campaña y podremos regresar a nuestros cuarteles de invierno, donde nos aguardan las nuevas tropas que continuamente se forman en Francia; y entonces la paz que firme será digna de mi pueblo, de vosotros y de mí.\n",
      "Napoleón.\n",
      "\n",
      "Source: {'start_index': 1597102, 'source': '../data/Guerra_y_paz_cleaned.txt'}\n",
      "Content: Lo único que se proponía era, evidentemente, engrandecer su propia persona y ofender a Alejandro: es decir, lo que al comienzo de la entrevista con Bálashov no deseaba en manera alguna.\n",
      "—Dicen que han llegado ustedes a una paz con los turcos, ¿es verdad?\n",
      "Bálashov inclinó afirmativamente la cabeza.\n",
      "—Se ha firmado la paz…— comenzó.\n",
      "Pero Napoleón no lo dejó seguir. Necesitaba hablar él solo y proseguía su discurso con elocuencia e irritación no contenida a la que son proclives las personas mimadas por la fortuna.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "En \"Guerra y Paz\" de León Tolstói, Napoleón es retratado como una figura central, pero no siempre de manera heroica. Se le describe en momentos de solemnidad, como el aniversario de su coronación, donde se muestra tranquilo, jovial y seguro de sí mismo, contemplando el campo de batalla. También se le ve dando órdenes a sus mariscales, instándolos a vencer a los \"mercenarios de Inglaterra\" para asegurar la paz. Sin embargo, Tolstói también muestra su lado más vanidoso y egocéntrico, como cuando interrumpe bruscamente a Bálashov durante una conversación, mostrando una elocuencia irritada y una necesidad de engrandecer su propia persona.\n",
      "\n",
      "En resumen, Napoleón en \"Guerra y Paz\" es presentado como un líder militar importante, pero también se exploran sus rasgos de personalidad, incluyendo su confianza, su ambición y su vanidad.\n"
     ]
    }
   ],
   "source": [
    "#agent.invoke({\"question\": \"¿Quién es Napoleón en el libro Guerra y Paz?\"})\n",
    "\n",
    "query = (\n",
    "    \"¿Quién es Napoleón en el libro Guerra y Paz?\"\n",
    ")\n",
    "\n",
    "for event in agent.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"¿Quién es Napoleón en el libro Guerra y Paz?\"}]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'En \"Guerra y Paz\", Napoleón es presentado como una figura central, aunque a menudo distante, en los eventos históricos que se desarrollan en la novela. Se le describe en momentos clave, como en el campo de batalla, donde su presencia irradia confianza y determinación. Por ejemplo, en el aniversario de su coronación, se le ve tranquilo y seguro de sí mismo, observando el amanecer antes de dirigirse a la batalla, con la convicción de que la victoria es posible.\\n\\nTambién se muestra su faceta de líder, dando órdenes y motivando a sus tropas. En una de sus intervenciones, Napoleón insta a sus soldados a no romper filas y a vencer a los \"mercenarios de Inglaterra\", con la promesa de un regreso a los cuarteles de invierno y una paz digna.\\n\\nSin embargo, la novela también expone un lado más personal y, a veces, vanidoso de Napoleón. En una interacción, se le describe como alguien que busca engrandecer su propia persona y ofender a Alejandro, mostrando una elocuencia y una irritación contenida, características de alguien acostumbrado a salirse con la suya.\\n\\nEn resumen, Napoleón en \"Guerra y Paz\" es retratado como un líder militar y político de gran influencia, cuya presencia marca el curso de la historia, pero también se vislumbran sus rasgos de carácter más íntimos y complejos.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response[\"messages\"][-1].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGoogleGenerativeAI(model='models/gemini-2.5-flash-lite', google_api_key=SecretStr('**********'), client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x000001B4F852AB50>, default_metadata=(), model_kwargs={})"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

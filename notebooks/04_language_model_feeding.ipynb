{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 04: Chunking, tokenizing and modeling\n",
    "\n",
    "This notebook is dedicated to preparing and feeding the cleaned book data to a language model for further processing, such as fine-tuning or generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Cleaned Data\n",
    "\n",
    "First, we will load the cleaned text data of the book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded text with 3153561 characters.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_path = \"../data/Guerra_y_paz_cleaned.txt\"\n",
    "with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    book_text = f.read()\n",
    "\n",
    "print(f\"Loaded text with {len(book_text)} characters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize Language Model and Tokenizer\n",
    "\n",
    "Here, we will load a pre-trained language model and its corresponding tokenizer. This section will be updated based on the specific model chosen (e.g., Hugging Face Transformers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 32000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "spm.SentencePieceTrainer.train(\n",
    "    input='..\\\\data\\\\Guerra_y_paz_cleaned.txt',  # filename, one sentence per line\n",
    "    model_prefix='..\\\\tokenizers\\\\wp_sp',        # output file name prefix\n",
    "    vocab_size=vocab_size,                      \n",
    "    model_type='bpe',                      # model_type: unigram, bpe, char, word\n",
    "    character_coverage=0.9995,             \n",
    "    user_defined_symbols=['<pad>', '<bos>', '<eos>']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\josep\\\\Desktop\\\\Tolstoi\\\\notebooks'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 32000\n",
      "ID de <bos>: 4\n"
     ]
    }
   ],
   "source": [
    "sp = spm.SentencePieceProcessor(model_file='..\\\\tokenizers\\\\wp_sp.model')\n",
    "\n",
    "print(\"Vocab size:\", sp.get_piece_size())\n",
    "print(\"ID de <bos>:\", sp.piece_to_id(\"<bos>\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.piece_to_id(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'▁e'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.id_to_piece(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 1139, 113, 995, 2678, 35, 484, 31943, 5]\n",
      "<bos> Era una noche tranquila en Moscú.<eos>\n"
     ]
    }
   ],
   "source": [
    "text = \"Era una noche tranquila en Moscú.\"\n",
    "ids = [4]+sp.encode(text, out_type=int)+[5]\n",
    "print(ids)\n",
    "print(sp.decode(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def make_batches(paragraphs, sp, seq_len=256, stride=128, add_bos=True):\n",
    "    \"\"\"\n",
    "    Convert paragraphs into padded tensor batches for training.\n",
    "\n",
    "    Args:\n",
    "        paragraphs (list[str]): Each paragraph is one line of text.\n",
    "        sp (SentencePieceProcessor): Trained tokenizer.\n",
    "        seq_len (int): Max sequence length (tokens per sample).\n",
    "        stride (int): Overlap stride between consecutive sequences. (slide window)\n",
    "        add_bos (bool): Whether to prepend <bos> to each paragraph.\n",
    "\n",
    "    Returns:\n",
    "        samples (list[torch.Tensor]): Each element is a LongTensor of token IDs.\n",
    "    \"\"\"\n",
    "    bos_id = sp.piece_to_id(\"<bos>\")\n",
    "    eos_id = sp.piece_to_id(\"<eos>\")\n",
    "    pad_id = sp.piece_to_id(\"<pad>\")\n",
    "\n",
    "    # Tokenize all paragraphs into one flat list of token IDs\n",
    "    tokens = []\n",
    "    for p in paragraphs:\n",
    "        if not p.strip():\n",
    "            continue\n",
    "        piece_ids = sp.encode(p, out_type=int)\n",
    "        if add_bos:\n",
    "            tokens.extend([bos_id] + piece_ids + [eos_id])\n",
    "        else:\n",
    "            tokens.extend(piece_ids + [eos_id])\n",
    "\n",
    "    # Slice into overlapping fixed-length chunks\n",
    "    samples = [\n",
    "        tokens[i:i+seq_len]\n",
    "        for i in range(0, len(tokens) - seq_len, stride)\n",
    "    ]\n",
    "\n",
    "    # Convert to tensors\n",
    "    tensors = [torch.tensor(s, dtype=torch.long) for s in samples]\n",
    "\n",
    "    # Build input/output pairs (shifted)\n",
    "    x_tensors = [t[:-1] for t in tensors]\n",
    "    y_tensors = [t[1:] for t in tensors]\n",
    "\n",
    "    # Pad sequences for batching later\n",
    "    def collate(batch):\n",
    "        xs, ys = zip(*batch)\n",
    "        xs_pad = pad_sequence(xs, batch_first=True, padding_value=pad_id)\n",
    "        ys_pad = pad_sequence(ys, batch_first=True, padding_value=pad_id)\n",
    "        return xs_pad, ys_pad\n",
    "\n",
    "    # return everything you need\n",
    "    return x_tensors, y_tensors, collate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare Data for Model Input\n",
    "\n",
    "The cleaned text needs to be tokenized and formatted into a suitable input for the language model. This may involve splitting into chunks, adding special tokens, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "class LMDataset(Dataset):\n",
    "    def __init__(self, x_tensors, y_tensors):\n",
    "        self.xs = x_tensors\n",
    "        self.ys = y_tensors\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.xs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.xs[idx], self.ys[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_paragraphs = book_text.splitlines()\n",
    "\n",
    "x_tensors, y_tensors, collate_fn = make_batches(book_paragraphs, sp, seq_len=256, stride=128, add_bos=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 5260  -> train: 4734, dev: 526\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# split params\n",
    "seed = 42\n",
    "train_frac = 0.9\n",
    "batch_size = 32  # adjust for GPU memory\n",
    "\n",
    "\n",
    "n = len(x_tensors)\n",
    "indices = list(range(n))\n",
    "random.Random(seed).shuffle(indices)\n",
    "split_idx = int(n * train_frac)\n",
    "train_idx, dev_idx = indices[:split_idx], indices[split_idx:]\n",
    "\n",
    "# create split lists\n",
    "x_train_tensors = [x_tensors[i] for i in train_idx]\n",
    "y_train_tensors = [y_tensors[i] for i in train_idx]\n",
    "x_dev_tensors   = [x_tensors[i] for i in dev_idx]\n",
    "y_dev_tensors   = [y_tensors[i] for i in dev_idx]\n",
    "\n",
    "print(f'Total samples: {n}  -> train: {len(x_train_tensors)}, dev: {len(x_dev_tensors)}')\n",
    "\n",
    "train_dataset = LMDataset(x_train_tensors, y_train_tensors)\n",
    "dev_dataset   = LMDataset(x_dev_tensors, y_dev_tensors)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "dev_loader   = DataLoader(dev_dataset,   batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# make dev_lines available for the training loop check later\n",
    "dev_lines = x_dev_tensors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feed to Language Model\n",
    "\n",
    "Finally, the prepared data will be fed to the language model. This could be for inference, fine-tuning, or other tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GRULanguageModel(nn.Module):\n",
    "    def __init__(self, n_tokens, emb_size=128, hid_size=256, num_layers=2, pad_id=None, eos_id=None):\n",
    "        super().__init__()\n",
    "        self.n_tokens = n_tokens\n",
    "        self.pad_id = pad_id\n",
    "        self.eos_id = eos_id\n",
    "\n",
    "        self.emb = nn.Embedding(n_tokens, emb_size, padding_idx=pad_id)\n",
    "        self.gru = nn.GRU(emb_size, hid_size, num_layers=num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hid_size, n_tokens)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        \"\"\"\n",
    "        x: [batch, seq_len] token indices\n",
    "        hidden: optional initial hidden state\n",
    "        returns:\n",
    "            logits: [batch, seq_len, n_tokens]\n",
    "            hidden: final hidden state\n",
    "        \"\"\"\n",
    "        emb = self.emb(x)                  # [B, L, emb_size]\n",
    "        output, hidden = self.gru(emb, hidden)  # [B, L, hid_size]\n",
    "        logits = self.linear(output)       # [B, L, n_tokens]\n",
    "        return logits, hidden\n",
    "\n",
    "    def generate(self, prefix_ids, max_len=100, temperature=1.0, device=\"cpu\"):\n",
    "        \"\"\"\n",
    "        Autoregressive generation\n",
    "        prefix_ids: tensor [1, L] of token IDs\n",
    "        returns: list of generated token IDs (including prefix)\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        prefix_ids = prefix_ids.to(device)\n",
    "        generated = prefix_ids.clone()\n",
    "        hidden = None\n",
    "\n",
    "        for _ in range(max_len):\n",
    "            logits, hidden = self.forward(generated[:, -1:], hidden)\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            generated = torch.cat([generated, next_token], dim=1)\n",
    "\n",
    "            # stop if EOS appears\n",
    "            if next_token.item() in (self.pad_id, self.eos_id):  # or eos_id if defined separately\n",
    "                break\n",
    "        return generated[0].tolist()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GRULanguageModel(n_tokens=sp.get_piece_size(), pad_id=sp.piece_to_id(\"<pad>\"), eos_id=sp.piece_to_id(\"<eos>\"))\n",
    "num_epochs = 3\n",
    "opt = torch.optim.Adam(model.parameters())\n",
    "\n",
    "train_history = []             # list of (step, loss) tuples appended during training\n",
    "dev_history = []               # list of (step, dev_loss) tuples appended when scoring dev set\n",
    "score_dev_every = 500          # evaluate dev set every N training steps (adjust to taste)\n",
    "\n",
    "bos_id = sp.piece_to_id(\"<bos>\")\n",
    "eos_id = sp.piece_to_id(\"<eos>\")\n",
    "pad_id = sp.piece_to_id(\"<pad>\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\josep\\AppData\\Local\\Temp\\ipykernel_21208\\4123249507.py:34: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.\n",
      "Consider using tensor.detach() first. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\autograd\\generated\\python_variable_methods.cpp:837.)\n",
      "  train_history.append((len(train_history), float(loss)))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from tqdm import trange\n",
    "\n",
    "# Assuming you have:\n",
    "# - dataset & loader from make_batches()\n",
    "# - model: GRULanguageModel\n",
    "# - train_history, dev_history: lists\n",
    "# - score_dev_every: int\n",
    "# - generate(): method in your model\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for i, (x_batch, y_batch) in enumerate(train_loader):\n",
    "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        logits, _ = model(x_batch)\n",
    "        loss = torch.nn.functional.cross_entropy(\n",
    "            logits.view(-1, logits.size(-1)),\n",
    "            y_batch.view(-1),\n",
    "            ignore_index=pad_id\n",
    "        )\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        opt.step()\n",
    "\n",
    "        train_history.append((len(train_history), float(loss)))\n",
    "        running_loss += float(loss)\n",
    "\n",
    "        # --- Visualization and sample generation ---\n",
    "        if (i + 1) % 50 == 0:\n",
    "            clear_output(True)\n",
    "            plt.figure(figsize=(6, 4))\n",
    "            plt.scatter(*zip(*train_history), alpha=0.1, label='train_loss')\n",
    "            if len(dev_history):\n",
    "                plt.plot(*zip(*dev_history), color='red', label='dev_loss')\n",
    "            plt.legend()\n",
    "            plt.grid()\n",
    "            plt.show()\n",
    "\n",
    "            print(\"Generated examples (temperature=0.5):\")\n",
    "            for _ in range(3):\n",
    "                # Start generation with <bos>\n",
    "                prefix = torch.tensor([[bos_id]], dtype=torch.long, device=device)\n",
    "                gen_ids = model.generate(prefix, max_len=100, temperature=0.5, device=device)\n",
    "                print(sp.decode(gen_ids))\n",
    "\n",
    "        # --- Evaluate on dev set periodically ---\n",
    "        if (i + 1) % score_dev_every == 0 and len(dev_lines) > 0:\n",
    "            print(\"Scoring dev set...\")\n",
    "            dev_loss = 0.0\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for x_dev, y_dev in dev_loader:\n",
    "                    x_dev, y_dev = x_dev.to(device), y_dev.to(device)\n",
    "                    logits, _ = model(x_dev)\n",
    "                    loss_dev = torch.nn.functional.cross_entropy(\n",
    "                        logits.view(-1, logits.size(-1)),\n",
    "                        y_dev.view(-1),\n",
    "                        ignore_index=pad_id\n",
    "                    )\n",
    "                    dev_loss += float(loss_dev) * x_dev.size(0)\n",
    "            dev_loss /= len(dev_loader.dataset)\n",
    "            dev_history.append((len(train_history), dev_loss))\n",
    "            print(f'Dev loss: {dev_loss:.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
